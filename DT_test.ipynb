{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from math import log\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fr = open('E:\\\\data\\\\machinelearninginaction\\\\Ch03\\\\lenses.txt') \n",
    "lenses = [inst.strip().split('\\t') for inst in fr.readlines()] \n",
    "lensesLabels = ['age', 'prescript', 'astigmatic', 'tearRate'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calcShannonEnt(dataSet): \n",
    "    numEntries = len(dataSet) \n",
    "    # 创建一个空字典 \n",
    "    labelCounts = {} \n",
    "    # for循环：使labelCounts字典保存多个键值对，并且以dataSet中数据的类别（标签）为键，该类别数据的条数为对应的值 \n",
    "    for featVec in dataSet: \n",
    "        currentLabel = featVec[-1] \n",
    "        if currentLabel not in labelCounts.keys(): \n",
    "            # keys()方法返回字典中的键 \n",
    "            labelCounts[currentLabel] = 0 \n",
    "            # 如果labelCounts中没有currentLabel，则添加一个以currentLabel为键的键值对，值为0 \n",
    "        labelCounts[currentLabel] += 1 \n",
    "            # 将labelCounts中类型为currentLabel值加1 \n",
    "    print(labelCounts)\n",
    "    shannonEnt = 0.0 \n",
    "    for key in labelCounts: \n",
    "        # 根据熵的公式进行累加 \n",
    "        prob = float(labelCounts[key])/numEntries \n",
    "        # 计算每种数据类别出现的概率 \n",
    "        shannonEnt -= prob * log(prob, 2) \n",
    "    return shannonEnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "按照给定特征划分数据集 \n",
    "dataSet：给定数据集 \n",
    "axis：给定特征所在特征向量的列 \n",
    "value：给定特征的特征值\n",
    "返回retDataSet：划分后的数据集\n",
    "''' \n",
    "def splitDataSet(dataSet, axis, value): \n",
    "    retDataSet = [] \n",
    "    for featVec in dataSet: \n",
    "        if featVec[axis] == value: \n",
    "            # 若当前特征向量指定特征列（第axis列，列从0开始）的特征值与给定的特征值（value）相等 \n",
    "            # 下面两行代码相当于将axis列去掉 \n",
    "            reducedFeatVec = featVec[:axis] \n",
    "            # 取当前特征向量axis列之前的列的特征 \n",
    "            reducedFeatVec.extend(featVec[axis+1:]) \n",
    "            # 将上一句代码取得的特征向量又加上axis列后的特征 \n",
    "            retDataSet.append(reducedFeatVec) \n",
    "            # 将划分后的特征向量添加到retDataSet中\n",
    "    return retDataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "选择最好的数据划分方式 \n",
    "dataSet：要进行划分的数据集 \n",
    "返回bestFeature：在分类时起决定性作用的特征（下标） \n",
    "'''\n",
    "def chooseBestFeatureToSplit(dataSet): \n",
    "    numFeatures = len(dataSet[0]) - 1 \n",
    "    # 特征的数量 \n",
    "    baseEntropy = calcShannonEnt(dataSet) \n",
    "    # 计算数据集的香农熵 \n",
    "    bestInfoGain = 0.0 \n",
    "    # bestInfoGain=0：最好的信息增益为0，表示再怎么划分，香农熵（信息熵）都不会再变化，这就是划分的最优情况 \n",
    "    bestFeature = -1 \n",
    "    for i in range(numFeatures): \n",
    "        # 根据数据的每个特征进行划分，并计算熵，熵减少最多的情况为最优，此时对数据进行划分的特征作为划分的最优特征 \n",
    "        featList = [example[i] for example in dataSet]\n",
    "        # featList为第i列数据（即第i个特征的所有特征值的列表（有重复）） \n",
    "        uniqueVals = set(featList) \n",
    "        # uniqueVals为第i列特征的特征值（不重复，例如有特征值1,1,0,0，uniqueVals为[0, 1]） \n",
    "        newEntropy = 0.0 \n",
    "        for value in uniqueVals: \n",
    "            subDataSet = splitDataSet(dataSet, i, value) \n",
    "            prob = len(subDataSet)/float(len(dataSet)) \n",
    "            newEntropy += prob * calcShannonEnt(subDataSet) \n",
    "            # newEntropy为将数据集根据第i列特征进行划分的 \n",
    "            # 所有子集的熵乘以该子集占总数据集比例的和 \n",
    "            infoGain = baseEntropy - newEntropy \n",
    "            # 计算信息增益，即熵减 \n",
    "            if infoGain > bestInfoGain: \n",
    "                bestInfoGain = infoGain \n",
    "                bestFeature = i \n",
    "    return bestFeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 当数据集已经处理了所有属性，但是分类标签依然不唯一时，采用多数表决的方法决定叶子结点的分类 \n",
    "def majorityCnt(classList): \n",
    "    classCount = {} \n",
    "    for vote in classList: \n",
    "        if vote not in classCount.keys(): \n",
    "            classCount[vote] = 0 \n",
    "            classCount[vote] += 1 \n",
    "            sortedClassCount = sorted(classCount.items(), key=operator.itemgetter(1), reverse=True) \n",
    "    return sortedClassCount[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "利用函数递归创建决策树 \n",
    "dataSet：数据集\n",
    "labels：标签列表，包含了数据集中所有特征的标签 \n",
    "'''\n",
    "def createTree(dataSet, labels): \n",
    "    classList = [example[-1] for example in dataSet]   # 取出dataSet最后一列的数据 \n",
    "    if classList.count(classList[0]) == len(classList):\n",
    "        # classList中classList[0]出现的次数=classList长度，表示类别完全相同，停止继续划分 \n",
    "        return classList[0] \n",
    "    if len(dataSet[0]) == 1: \n",
    "        # 遍历完所有特征时返回出现次数最多的类别 \n",
    "        return majorityCnt(classList) \n",
    "    bestFeat = chooseBestFeatureToSplit(dataSet)     # 计算划分的最优特征（下标） \n",
    "    bestFeatLabel = labels[bestFeat]     # 数据划分的最优特征的标签（即是什么特征） \n",
    "    myTree = {bestFeatLabel:{}}          # 创建一个树（字典），bestFeatLabel为根结点 \n",
    "    del(labels[bestFeat]) \n",
    "    featValues = [example[bestFeat] for example in dataSet] \n",
    "    uniqueVals = set(featValues) \n",
    "    for value in uniqueVals: \n",
    "        subLabels = labels[:] \n",
    "        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), subLabels)  # 利用递归构造决策树 \n",
    "    return myTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义文本框和箭头格式 \n",
    "decisionNode = dict(boxstyle=\"sawtooth\", fc=\"0.8\") \n",
    "leafNode = dict(boxstyle=\"round4\", fc=\"0.8\") \n",
    "arrow_args = dict(arrowstyle=\"<-\") \n",
    "# 使用文本注解绘制树节点 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotNode(nodeText, centerPt, parentPt, nodeType): \n",
    "    createPlot.ax1.annotate(nodeText, xy=parentPt, xycoords='axes fraction', xytext=centerPt, textcoords='axes fraction', \n",
    "                            va=\"center\", ha=\"center\", bbox=nodeType, arrowprops=arrow_args) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createPlot(): \n",
    "    fig = plt.figure(1, facecolor='white') \n",
    "    fig.clf() \n",
    "    createPlot.ax1 = plt.subplot(111, frameon=False) \n",
    "    plotNode('a decision node', (0.5, 0.1), (0.1, 0.5), decisionNode) \n",
    "    plotNode('a leaf node', (0.8, 0.1), (0.3, 0.8), leafNode) \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 获取叶节点的数目，以便确定x轴的长度 \n",
    "def getNumLeafs(myTree): \n",
    "    numLeafs = 0 \n",
    "    firstStr = list(myTree.keys())[0] #根结点 \n",
    "    secondDict = myTree[firstStr] \n",
    "    for key in secondDict.keys(): \n",
    "        if type(secondDict[key]).__name__ == 'dict': \n",
    "            numLeafs += getNumLeafs(secondDict[key]) \n",
    "        else: \n",
    "            numLeafs += 1 \n",
    "    return numLeafs # 获取决策树的深度 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTreeDepth(myTree): \n",
    "    maxDepth = 0 \n",
    "    firstStr = list(myTree.keys())[0] \n",
    "    secondDict = myTree[firstStr] \n",
    "    for key in secondDict.keys(): \n",
    "        if type(secondDict[key]).__name__ == 'dict': \n",
    "            thisDepth = 1 + getTreeDepth(secondDict[key]) \n",
    "        else: \n",
    "            thisDepth = 1 \n",
    "        if thisDepth > maxDepth: \n",
    "            maxDepth = thisDepth \n",
    "    return maxDepth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 在父子节点间填充文本信息 \n",
    "def plotMidText(cntrPt, parentPt, textString): \n",
    "    xMid = (parentPt[0] - cntrPt[0]) / 2.0 + cntrPt[0] \n",
    "    yMid = (parentPt[1] - cntrPt[1]) / 2.0 + cntrPt[1] \n",
    "    createPlot.ax1.text(xMid, yMid, textString) \n",
    "    \n",
    "def plotTree(myTree, parentPt, nodeText): \n",
    "    numLeafs = getNumLeafs(myTree) \n",
    "    depth = getTreeDepth(myTree) \n",
    "    firstStr = list(myTree.keys())[0] \n",
    "    cntrPt = (plotTree.xOff + (1.0 + float(numLeafs)) / 2.0 / plotTree.totalW, plotTree.yOff) \n",
    "    plotMidText(cntrPt, parentPt, nodeText) \n",
    "    plotNode(firstStr, cntrPt, parentPt, decisionNode) \n",
    "    secondDict = myTree[firstStr] \n",
    "    plotTree.yOff = plotTree.yOff - 1.0/plotTree.totalD \n",
    "    for key in secondDict.keys(): \n",
    "        if type(secondDict[key]).__name__ == 'dict': \n",
    "            plotTree(secondDict[key], cntrPt, str(key)) \n",
    "        else: \n",
    "            plotTree.xOff = plotTree.xOff + 1.0/plotTree.totalW \n",
    "            plotNode(secondDict[key], (plotTree.xOff, plotTree.yOff), cntrPt, leafNode) \n",
    "            plotMidText((plotTree.xOff, plotTree.yOff), cntrPt, str(key)) \n",
    "    plotTree.yOff = plotTree.yOff + 1.0/plotTree.totalD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createPlot(inTree): \n",
    "    fig = plt.figure(1, facecolor='white') \n",
    "    fig.clf() \n",
    "    axprops = dict(xticks=[], yticks=[]) \n",
    "    createPlot.ax1 = plt.subplot(111, frameon=False, **axprops) \n",
    "    plotTree.totalW = float(getNumLeafs(inTree)) \n",
    "    plotTree.totalD = float(getTreeDepth(inTree)) \n",
    "    plotTree.xOff = -0.5/plotTree.totalW\n",
    "    plotTree.yOff = 1.0 \n",
    "    plotTree(inTree, (0.5, 1.0), '') \n",
    "    plt.show()\n",
    "\n",
    "# 使用决策树的分类函数 \n",
    "def classify(inputTree, featLabels, testVec): \n",
    "    firstStr = list(inputTree.keys())[0] \n",
    "    secondDict = inputTree[firstStr] \n",
    "    featIndex = featLabels.index(firstStr) \n",
    "    for key in secondDict.keys(): \n",
    "        if testVec[featIndex] == key: \n",
    "            if type(secondDict[key]).__name__ == 'dict': \n",
    "                classLabel = classify(secondDict[key], featLabels, testVec) \n",
    "            else: \n",
    "                classLabel = secondDict[key] \n",
    "    return classLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 使用pickle模块存储决策树 \n",
    "def storeTree(inputTree, filename): \n",
    "    import pickle \n",
    "    fw = open(filename, 'w') \n",
    "    pickle.dump(inputTree, fw) \n",
    "    fw.close() \n",
    "    \n",
    "def grabTree(filename): \n",
    "    import pickle \n",
    "    fr = open(filename) \n",
    "    return pickle.load(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'no lenses': 15, 'soft': 5, 'hard': 4}\n",
      "{'no lenses': 4, 'soft': 2, 'hard': 2}\n",
      "{'no lenses': 5, 'soft': 2, 'hard': 1}\n",
      "{'no lenses': 6, 'soft': 1, 'hard': 1}\n",
      "{'no lenses': 8, 'soft': 3, 'hard': 1}\n",
      "{'no lenses': 7, 'soft': 2, 'hard': 3}\n",
      "{'no lenses': 8, 'hard': 4}\n",
      "{'no lenses': 7, 'soft': 5}\n",
      "{'no lenses': 12}\n",
      "{'hard': 4, 'soft': 5, 'no lenses': 3}\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-5d7d96587adb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlensesTree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreateTree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlenses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlensesLabels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 画决策树\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mcreatePlot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlensesTree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlenses\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-39682f394b65>\u001b[0m in \u001b[0;36mcreateTree\u001b[1;34m(dataSet, labels)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmajorityCnt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassList\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mbestFeat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchooseBestFeatureToSplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataSet\u001b[0m\u001b[1;33m)\u001b[0m     \u001b[1;31m# 计算划分的最优特征（下标）\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mbestFeatLabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbestFeat\u001b[0m\u001b[1;33m]\u001b[0m     \u001b[1;31m# 数据划分的最优特征的标签（即是什么特征）\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mmyTree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mbestFeatLabel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m}\u001b[0m          \u001b[1;31m# 创建一个树（字典），bestFeatLabel为根结点\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mdel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbestFeat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "lensesTree = createTree(lenses, lensesLabels) # 画决策树 \n",
    "createPlot(lensesTree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
