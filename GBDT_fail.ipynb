{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import  accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_datapath = 'E:\\\\code\\\\python\\\\qoe_model\\\\raw_data\\\\3w_data.csv'\n",
    "data = pd.read_csv(raw_datapath)\n",
    "\n",
    "column_names = ['InitialBufferTime', 'VideoPlayDuration','StallingRatio', 'VIDEO_BITRATE', 'VIDEO_CLARITY', 'VIDEO_ALL_PEAK_RATE', \n",
    "                'VIDEO_AVERAGE_RATE', 'USERBUFFERTIME', 'VIDEOSIZE', 'SCREEN_RESOLUTION_LONG', 'VIDEO_BUFFERING_PEAK_RATE', \n",
    "                'EVMOS', 'ELOADING', 'ESTALLING', 'USER_SCORE']\n",
    "#########################################################\n",
    "############ 将 name 列的离散数据进行编号 ###############\n",
    "#########################################################\n",
    "def class_normalization(name, X):\n",
    "    \n",
    "    # name不是list,是str\n",
    "    a = X[name]\n",
    "    b = a.value_counts()\n",
    "    c = b.index\n",
    "\n",
    "    list1 = []\n",
    "    list2 = []\n",
    "    for i in range(len(c)):\n",
    "        list1.append(i)\n",
    "        list2.append(c[i])\n",
    "        \n",
    "    b = a.replace(list2, list1)\n",
    "    \n",
    "    data1 = X.drop([name], axis=1)\n",
    "    data1.insert(2, name, b)\n",
    "    \n",
    "    return data1\n",
    "\n",
    "##########################################################\n",
    "#################### 移除 name 列 ########################\n",
    "##########################################################\n",
    "def remove_col(name, all_name):\n",
    "    \n",
    "    list = []\n",
    "    for i in range(len(column_names)):\n",
    "        if column_names[i] != name:\n",
    "            list.append(column_names[i])\n",
    "    return list\n",
    "\n",
    "# 生成每一个batch\n",
    "def generatebatch(X,Y,n_examples, batch_size): \n",
    "    for batch_i in range(n_examples // batch_size): \n",
    "        start = batch_i * batch_size \n",
    "        end = start + batch_size \n",
    "        batch_xs = X[start:end] \n",
    "        batch_ys = Y[start:end] \n",
    "        yield batch_xs, batch_ys \n",
    "\n",
    "name = 'VIDEO_CLARITY'\n",
    "data1 = class_normalization(name, data)\n",
    "data1 = shuffle(data1)\n",
    "data1 = data1.reset_index(drop = True)\n",
    "\n",
    "X1 = data1[remove_col(name, column_names)]\n",
    "X2 = data1[[name]]\n",
    "\n",
    "# 选取第i个分数\n",
    "Y1 = data1[column_names[11]]\n",
    "Y2 = data1[column_names[12]]\n",
    "Y3 = data1[column_names[13]]\n",
    "Y4 = data1[column_names[14]]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X1_data = scaler.fit_transform(X1)\n",
    "X2_data = OneHotEncoder().fit_transform(X2.values.reshape(-1, 1)).todense()\n",
    "X_data = np.hstack((X1_data, X2_data)).getA()\n",
    "\n",
    "# 随机采样25%的数据用于测试，剩下的75%用于构建训练集合。\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X_data, Y1, test_size=0.25, random_state = 33)\n",
    "\n",
    "#X_train = X_train.reshape(-1,8,8,1)\n",
    "#X_test = X_test.reshape(-1,8,8,1)\n",
    "\n",
    "X1_train = X1_train.tolist()\n",
    "X1_test = X1_test.tolist()\n",
    "y1_train = OneHotEncoder().fit_transform(y1_train.values.reshape(-1, 1)).todense().getA().tolist()\n",
    "y1_test_code = OneHotEncoder().fit_transform(y1_test.values.reshape(-1, 1)).todense().getA().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sat Dec 15 15:25:03 2018\n",
    "\n",
    "@author: lj\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "class node:\n",
    "    # 树的节点的类\n",
    "    def __init__(self, fea=-1, value=None, results=None, right=None, left=None):\n",
    "        self.fea = fea  # 用于切分数据集的属性的列索引值\n",
    "        self.value = value  # 设置划分的值\n",
    "        self.results = results  # 存储叶节点的值\n",
    "        self.right = right  # 右子树\n",
    "        self.left = left  # 左子树\n",
    "\n",
    "class CART_RT(object):\n",
    "    # CART算法的类\n",
    "    def __init__(self,data_X,data_Y,min_sample, min_err):\n",
    "        # 初始化CART类参数\n",
    "        self.data_X = data_X #待回归样本数据的特征\n",
    "        self.data_Y = data_Y #待回归样本数据的标签\n",
    "        self.min_sample = min_sample # 每个叶节点最多的样本数\n",
    "        self.min_err = min_err #最小方差\n",
    "        \n",
    "    def fit(self):\n",
    "        '''\n",
    "            构建树\n",
    "            input:  data(list):训练样本\n",
    "                    min_sample(int):叶子节点中最少的样本数\n",
    "                    min_err(float):最小的error\n",
    "            output: node:树的根结点\n",
    "        '''  \n",
    "        # 将样本特征与样本标签合成完整的样本\n",
    "        data = combine(self.data_X,self.data_Y)\n",
    "        # 构建决策树，函数返回该决策树的根节点\n",
    "        if len(data) <= self.min_sample:\n",
    "            return node(results=leaf(data))\n",
    "            \n",
    "        # 1、初始化\n",
    "        best_err = err_cnt(data)\n",
    "        bestCriteria = None  # 存储最佳切分属性以及最佳切分点\n",
    "        bestSets = None  # 存储切分后的两个数据集\n",
    "    \n",
    "        # 2、开始构建CART回归树\n",
    "        feature_num = len(data[0]) - 1\n",
    "        for fea in range(0, feature_num):\n",
    "            feature_values = {}\n",
    "            for sample in data:\n",
    "                feature_values[sample[fea]] = 1\n",
    "            for value in feature_values.keys():\n",
    "                # 2.1、尝试划分\n",
    "                (set_1, set_2) = split_tree(data, fea, value)\n",
    "                combine_set_1 = combine(set_1[0],set_1[1])\n",
    "                combine_set_2 = combine(set_2[0],set_2[1])\n",
    "                if len(combine_set_1) < 2 or len(combine_set_2) < 2:\n",
    "                    continue\n",
    "                # 2.2、计算划分后的error值\n",
    "                now_err = err_cnt(combine_set_1) + err_cnt(combine_set_2)\n",
    "                # 2.3、更新最优划分\n",
    "                if now_err < best_err and len(combine_set_1) > 0 and len(combine_set_2) > 0:\n",
    "                    best_err = now_err\n",
    "                    bestCriteria = (fea, value)\n",
    "                    bestSets = (set_1, set_2)\n",
    "\n",
    "        # 3、判断划分是否结束\n",
    "        if best_err > self.min_err:\n",
    "            right = CART_RT(bestSets[0][0],bestSets[0][1], self.min_sample, self.min_err).fit()\n",
    "            left = CART_RT(bestSets[1][0],bestSets[1][1], self.min_sample, self.min_err).fit()\n",
    "            return node(fea=bestCriteria[0], value=bestCriteria[1], right=right, left=left)\n",
    "        else:\n",
    "            return node(results=leaf(data))  # 返回当前的类别标签作为最终的类别标签\n",
    "\n",
    "def combine(data_X,data_Y):\n",
    "    '''样本特征与标签合并\n",
    "    input:data_X(list):样本特征\n",
    "          data_Y(list):样本标签\n",
    "    output:data(list):样本数据\n",
    "    '''\n",
    "    m = len(data_X)\n",
    "    data = copy.deepcopy(data_X)\n",
    "    for i in range(m):\n",
    "        data[i].append(data_Y[i])\n",
    "    return data\n",
    "        \n",
    "def err_cnt(data):\n",
    "    '''回归树的划分指标\n",
    "    input:  data(list):训练数据\n",
    "    output: m*s^2(float):总方差\n",
    "    '''\n",
    "    data = np.mat(data)\n",
    "    return np.var(data[:, -1]) * np.shape(data)[0]\n",
    "\n",
    "def split_tree(data, fea, value):\n",
    "    '''根据特征fea中的值value将数据集data划分成左右子树\n",
    "    input:  data(list):训练样本\n",
    "            fea(float):需要划分的特征index\n",
    "            value(float):指定的划分的值\n",
    "    output: (set_1, set_2)(tuple):左右子树的聚合\n",
    "    '''\n",
    "    set_1 = []  # 右子树的集合\n",
    "    set_2 = []  # 左子树的集合\n",
    "    tmp_11 = []\n",
    "    tmp_12 = []\n",
    "    tmp_21 = []\n",
    "    tmp_22 = []\n",
    "    for x in data:\n",
    "        if x[fea] >= value:\n",
    "            tmp_11.append(x[0:-1])\n",
    "            tmp_12.append(x[-1])\n",
    "        else:\n",
    "            tmp_21.append(x[0:-1])\n",
    "            tmp_22.append(x[-1])\n",
    "    set_1.append(tmp_11)\n",
    "    set_1.append(tmp_12)\n",
    "    set_2.append(tmp_21)\n",
    "    set_2.append(tmp_22)\n",
    "    return (set_1, set_2)\n",
    "\n",
    "def leaf(dataSet):\n",
    "    '''计算叶节点的值\n",
    "    input:  dataSet(list):训练样本\n",
    "    output: mean(data[:, -1])(float):均值\n",
    "    '''\n",
    "    data = np.mat(dataSet)\n",
    "    return np.mean(data[:, -1])\n",
    "\n",
    "def predict(sample, tree):\n",
    "    '''对每一个样本sample进行预测\n",
    "    input:  sample(list):样本\n",
    "            tree:训练好的CART回归树模型\n",
    "    output: results(float):预测值\n",
    "    '''\n",
    "    # 1、只是树根\n",
    "    if tree.results != None:\n",
    "        return tree.results\n",
    "    else:\n",
    "    # 2、有左右子树\n",
    "        val_sample = sample[tree.fea]  # fea处的值\n",
    "        branch = None\n",
    "        # 2.1、选择右子树\n",
    "        if val_sample >= tree.value:\n",
    "            branch = tree.right\n",
    "        # 2.2、选择左子树\n",
    "        else:\n",
    "            branch = tree.left\n",
    "        return predict(sample, branch)\n",
    "\n",
    "def cal_error(data_X,data_Y, tree):\n",
    "    ''' 评估CART回归树模型\n",
    "    input:  data(list):\n",
    "            tree:训练好的CART回归树模型\n",
    "    output: err/m(float):均方误差\n",
    "    '''\n",
    "    m = len(data_X)  # 样本的个数   \n",
    "    n = len(data_X[0])  # 样本中特征的个数\n",
    "    err = 0.0\n",
    "    for i in range(m):\n",
    "        tmp = []\n",
    "        for j in range(n):\n",
    "            tmp.append(data_X[i][j])\n",
    "        pre = predict(tmp, tree)  # 对样本计算其预测值\n",
    "        # 计算残差\n",
    "        err += (data_Y[i] - pre) * (data_Y[i] - pre)\n",
    "    return err / m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GBDT_RT(object):\n",
    "    '''\n",
    "    GBDT回归算法类\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.trees = None ##用于存放GBDT的树\n",
    "        self.learn_rate = learn_rate ## 学习率，防止过拟合\n",
    "        self.init_value = None ##初始数值\n",
    "        self.fn = lambda x: x\n",
    "        \n",
    "    def get_init_value(self,y):\n",
    "        '''\n",
    "        计算初始数值为平均值\n",
    "        input:y(list):样本标签列表\n",
    "        output:average(float):样本标签的平均值\n",
    "        '''\n",
    "        average = sum(y)/len(y)\n",
    "        return average\n",
    "    \n",
    "    def get_residuals(self,y,y_hat):\n",
    "        '''\n",
    "        计算样本标签标签与预测列表的残差\n",
    "        input:y(list):样本标签列表\n",
    "              y_hat(list):预测标签列表\n",
    "        output:y_residuals(list):样本标签标签与预测列表的残差\n",
    "        '''\n",
    "        y_residuals = []\n",
    "        for i in range(len(y)):\n",
    "            y_residuals.append(y[i] - y_hat[i])\n",
    "        return y_residuals\n",
    "    \n",
    "    def fit(self,data_X,data_Y,n_estimators,learn_rate,min_sample, min_err):\n",
    "        '''\n",
    "        训练GBDT模型\n",
    "        input:self(object):GBDT_RT类\n",
    "              data_X(list):样本特征\n",
    "              data_Y(list):样本标签\n",
    "              n_estimators(int):GBDT中CART树的个数\n",
    "              learn_rate(float):学习率\n",
    "              min_sample(int):学习CART时叶节点的最小样本数\n",
    "              min_err(float):学习CART时最小方差\n",
    "        '''\n",
    "        ## 初始化预测标签和残差\n",
    "        self.init_value = self.get_init_value(data_Y)\n",
    "        \n",
    "        n = len(data_Y)\n",
    "        y_hat = [self.init_value] * n ##初始化预测标签\n",
    "        y_residuals = self.get_residuals(data_Y,y_hat)\n",
    "        \n",
    "        self.trees = []\n",
    "        self.learn_rate = learn_rate\n",
    "        ## 迭代训练GBDT\n",
    "        for j in range(n_estimators):\n",
    "            idx = range(n)\n",
    "            X_sub = [data_X[i] for i in idx] ## 样本特征列表\n",
    "            residuals_sub = [y_residuals[i] for i in idx] ## 标签残差列表\n",
    "            \n",
    "            tree = CART_RT(X_sub,residuals_sub, min_sample, min_err).fit()\n",
    "            res_hat = [] ##残差的预测值\n",
    "            for m in range(n):\n",
    "                res_hat.append(predict(data_X[m],tree))\n",
    "            ## 计算此时的预测值等于原预测值加残差预测值\n",
    "            y_hat = [y_hat[i] + self.learn_rate * res_hat[i] for i in idx]\n",
    "            y_residuals = self.get_residuals(data_Y,y_hat)\n",
    "            self.trees.append(tree)\n",
    "            \n",
    "    def GBDT_predict(self,xi):\n",
    "        '''预测一个样本\n",
    "        '''\n",
    "        return self.fn(self.init_value + sum(self.learn_rate * predict(xi,tree) for tree in self.trees))\n",
    "    \n",
    "    def GBDT_predicts(self,X):\n",
    "        '''预测多个样本\n",
    "        '''\n",
    "        return [self.GBDT_predict(xi) for xi in X]\n",
    "\n",
    "def error(Y_test,predict_results):\n",
    "    '''计算预测误差\n",
    "    input:Y_test(list):测试样本标签\n",
    "          predict_results(list):测试样本预测值\n",
    "    output:error(float):均方误差\n",
    "    '''\n",
    "    Y = np.mat(Y_test)\n",
    "    results = np.mat(predict_results)\n",
    "    error = np.square(Y - results).sum() / len(Y_test)\n",
    "    return error\n",
    "\n",
    "def load_data(data_file):\n",
    "    '''\n",
    "    导入训练数据\n",
    "    input:  data_file(string):保存训练数据的文件\n",
    "    output: data(list):训练数据\n",
    "    '''\n",
    "    data_X = []\n",
    "    data_Y = []\n",
    "    f = open(data_file)\n",
    "    for line in f.readlines():\n",
    "        sample = []\n",
    "        lines = line.strip().split(\"\\t\")\n",
    "        data_Y.append(float(lines[-1]))\n",
    "        for i in range(len(lines) - 1):\n",
    "            sample.append(float(lines[i]))  # 转换成float格式\n",
    "        data_X.append(sample)\n",
    "    f.close()    \n",
    "    return data_X,data_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- 1.load data ----------------\n",
      "------------2.Parameters Setting-----------\n",
      "--------------3.build GBDT ---------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'int' and 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-2521127eb120>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"--------------3.build GBDT ---------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mgbdt_rt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGBDT_RT\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mgbdt_rt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX1_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my1_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlearn_rate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmin_sample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_err\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-------------4.Predict Result--------------'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-8ae10a5fee8d>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, data_X, data_Y, n_estimators, learn_rate, min_sample, min_err)\u001b[0m\n\u001b[0;32m     42\u001b[0m         '''\n\u001b[0;32m     43\u001b[0m         \u001b[1;31m## 初始化预测标签和残差\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_init_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_Y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_Y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-8ae10a5fee8d>\u001b[0m in \u001b[0;36mget_init_value\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0moutput\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0m样本标签的平均值\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         '''\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0maverage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'int' and 'list'"
     ]
    }
   ],
   "source": [
    "print (\"------------- 1.load data ----------------\")\n",
    "X_train = X1_train[0:150]\n",
    "Y_train = y1_train[0:150]\n",
    "X_test = X1_test[150:200]\n",
    "\n",
    "print('------------2.Parameters Setting-----------')\n",
    "n_estimators = 4\n",
    "learn_rate = 0.5\n",
    "min_sample = 30\n",
    "min_err = 0.3\n",
    "\n",
    "print (\"--------------3.build GBDT ---------------\")\n",
    "gbdt_rt = GBDT_RT()\n",
    "gbdt_rt.fit(X1_train,y1_train,n_estimators,learn_rate,min_sample, min_err)\n",
    "\n",
    "print('-------------4.Predict Result--------------')\n",
    "predict_results = gbdt_rt.GBDT_predicts(X1_test)\n",
    "\n",
    "print('--------------5.Predict Error--------------')\n",
    "error = error(y1_test,predict_results)\n",
    "print('Predict error is: ',error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.19035],\n",
       " [0.306657],\n",
       " [0.017568],\n",
       " [0.122328],\n",
       " [0.076274],\n",
       " [0.614127],\n",
       " [0.220722],\n",
       " [0.08943],\n",
       " [0.278817],\n",
       " [0.520287],\n",
       " [0.726976],\n",
       " [0.180485],\n",
       " [0.801524],\n",
       " [0.474273],\n",
       " [0.345116],\n",
       " [0.981951],\n",
       " [0.127349],\n",
       " [0.75712],\n",
       " [0.345419],\n",
       " [0.314532],\n",
       " [0.250828],\n",
       " [0.431255],\n",
       " [0.386669],\n",
       " [0.143794],\n",
       " [0.470839],\n",
       " [0.093065],\n",
       " [0.205377],\n",
       " [0.083329],\n",
       " [0.243475],\n",
       " [0.062389],\n",
       " [0.764116],\n",
       " [0.018287],\n",
       " [0.973603],\n",
       " [0.458826],\n",
       " [0.5112],\n",
       " [0.712587],\n",
       " [0.464745],\n",
       " [0.984328],\n",
       " [0.414291],\n",
       " [0.799551],\n",
       " [0.499037],\n",
       " [0.966757],\n",
       " [0.756594],\n",
       " [0.444938],\n",
       " [0.410167],\n",
       " [0.532335],\n",
       " [0.343909],\n",
       " [0.854302],\n",
       " [0.846882],\n",
       " [0.740758],\n",
       " [0.150668],\n",
       " [0.177606],\n",
       " [0.445289],\n",
       " [0.734653],\n",
       " [0.559488],\n",
       " [0.232311],\n",
       " [0.934435],\n",
       " [0.219089],\n",
       " [0.636525],\n",
       " [0.307605],\n",
       " [0.713198],\n",
       " [0.116343],\n",
       " [0.680737],\n",
       " [0.48473],\n",
       " [0.929408],\n",
       " [0.008507],\n",
       " [0.872161],\n",
       " [0.75553],\n",
       " [0.620671],\n",
       " [0.47226],\n",
       " [0.257488],\n",
       " [0.130654],\n",
       " [0.512333],\n",
       " [0.74771],\n",
       " [0.669948],\n",
       " [0.644856],\n",
       " [0.894206],\n",
       " [0.820471],\n",
       " [0.790796],\n",
       " [0.010729],\n",
       " [0.846777],\n",
       " [0.349175],\n",
       " [0.453662],\n",
       " [0.624017],\n",
       " [0.211074],\n",
       " [0.062555],\n",
       " [0.739709],\n",
       " [0.985896],\n",
       " [0.782088],\n",
       " [0.642561],\n",
       " [0.779007],\n",
       " [0.185631],\n",
       " [0.52525],\n",
       " [0.236802],\n",
       " [0.440958],\n",
       " [0.39758],\n",
       " [0.823146],\n",
       " [0.370173],\n",
       " [0.791675],\n",
       " [0.456647],\n",
       " [0.11335],\n",
       " [0.351074],\n",
       " [0.182684],\n",
       " [0.914034],\n",
       " [0.751486],\n",
       " [0.216572],\n",
       " [0.013273],\n",
       " [0.469726],\n",
       " [0.060676],\n",
       " [0.77631],\n",
       " [0.061648],\n",
       " [0.714077],\n",
       " [0.559264],\n",
       " [0.121876],\n",
       " [0.330586],\n",
       " [0.662909],\n",
       " [0.785142],\n",
       " [0.161352],\n",
       " [0.985215],\n",
       " [0.457734],\n",
       " [0.171574],\n",
       " [0.334277],\n",
       " [0.501065],\n",
       " [0.988736],\n",
       " [0.659242],\n",
       " [0.359861],\n",
       " [0.790434],\n",
       " [0.462458],\n",
       " [0.823012],\n",
       " [0.594668],\n",
       " [0.498207],\n",
       " [0.574882],\n",
       " [0.570048],\n",
       " [0.33157],\n",
       " [0.195407],\n",
       " [0.814327],\n",
       " [0.641925],\n",
       " [0.238778],\n",
       " [0.400138],\n",
       " [0.670479],\n",
       " [0.069076],\n",
       " [0.294373],\n",
       " [0.025628],\n",
       " [0.697772],\n",
       " [0.729626],\n",
       " [0.293071],\n",
       " [0.531802],\n",
       " [0.487338],\n",
       " [0.21578],\n",
       " [0.625818],\n",
       " [0.179389],\n",
       " [0.192552],\n",
       " [0.671661],\n",
       " [0.952391],\n",
       " [0.795133],\n",
       " [0.950494],\n",
       " [0.194894],\n",
       " [0.35146],\n",
       " [0.863456],\n",
       " [0.945221],\n",
       " [0.77984],\n",
       " [0.996606],\n",
       " [0.632184],\n",
       " [0.790898],\n",
       " [0.022503],\n",
       " [0.318983],\n",
       " [0.369633],\n",
       " [0.1573],\n",
       " [0.153223],\n",
       " [0.360068],\n",
       " [0.433917],\n",
       " [0.133461],\n",
       " [0.757252],\n",
       " [0.309391],\n",
       " [0.195586],\n",
       " [0.240259],\n",
       " [0.340591],\n",
       " [0.243436],\n",
       " [0.612755],\n",
       " [0.089407],\n",
       " [0.469695],\n",
       " [0.94356],\n",
       " [0.177241],\n",
       " [0.317756],\n",
       " [0.515337],\n",
       " [0.344773],\n",
       " [0.537029],\n",
       " [0.626878],\n",
       " [0.20894],\n",
       " [0.470697],\n",
       " [0.054448],\n",
       " [0.10923],\n",
       " [0.158325],\n",
       " [0.97665],\n",
       " [0.643441],\n",
       " [0.215841],\n",
       " [0.905337],\n",
       " [0.154354],\n",
       " [0.947922],\n",
       " [0.201391]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict = {}\n",
    "dict['you'] = ['a', 'n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'you': ['a', 'n']}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
